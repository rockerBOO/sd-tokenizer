<!doctype html>
<html lang="en">
  <head>
    <title>Diffusion Tokenizer</title>
    <meta content="text/html;charset=utf-8" http-equiv="Content-Type" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta
      name="description"
      content="Informs you about how your prompt/words gets turned into tokens, privately in your browser. For Diffusion models like Stable Diffusion, Flux, Pixart"
    />

    <link
      rel="icon"
      type="image/png"
      sizes="32x32"
      href="./assets/img/favicon-32.png"
    />
    <link rel="shortcut icon" href="./assets/img/favicon-32.ico" />
    <link
      rel="apple-touch-icon"
      sizes="180x180"
      href="./assets/img/apple-favicon.png"
    />
    <link rel="stylesheet" href="./assets/css/reset.css" />
    <link rel="stylesheet" href="./assets/css/main.css" />

    <script type="module" src="./assets/js/main.jsx"></script>
  </head>
  <body>
    <main id="app" class="container"></main>

    <article>
      <aside id="notes">
        <h3>Notes</h3>
        <h4>End of word character &lt;/w&gt;</h4>
        <p>
          End of word character (&lt;/w&gt;) is part of the token to represent a
          token that is at the end of the word. This is different than if it was
          at the start of word, or in the middle of a word. Used in CLIP
          tokenizer.
        </p>
        <h4>Start of word character ▁</h4>
        <p>
          Start of word character (▁) is part of the token to represent a token
          that is at the start of the word. This is different than if it was at
          the end of word, or in the middle of a word. Used in T5 tokenizer.
        </p>
        <h4>Token highlighting</h4>
        <p>
          Tokens have alternating background colors to represent how the words
          can be broken up as tokens.
        </p>
        <h4>How do we get the tokens?</h4>
        <p>
          We use the
          <a href="https://github.com/huggingface/tokenizers" target="_blank"
            >Tokenizer</a
          >
          library to extract the tokens from your text. We download the
          tokenizer.json from the relevant text encoder. These tokens are the
          same that are used when prompting diffusion models.
        </p>
        <h4 id="limit-for-tokens-in-stable-diffusion">
          What is the limit for tokens in Stable Diffusion?
        </h4>
        <p>
          Tokens are limited to 77 in CLIP Text Encoder models. 2 tokens are
          used to indicate the start and stop of the text input, leaving us with
          75 tokens to be used in prompting.
        </p>

        <p>
          To get around this limitation we can concatenate our tokens into
          smaller pieces. If we have 120 tokens, we can split it at 75 tokens
          and put the remaining 45 tokens in another.
        </p>

        <p>[75 tokens, 45 tokens]</p>

        <p>
          To prevent splitting at 75 tokens, image generation tools may have a
          way to set a BREAK point. This allows you to control where the break
          happens and create complete prompts.
        </p>

        <p>Example (a1111 webui):</p>
        <pre><code>a large dog BREAK a small cat</code></pre>
        <h4>
          What is the limit for tokens using Pixart or Flux models with T5xxl?
        </h4>
        <p>
          Tokens are limited to the max of 512 for the T5xxl model. The CLIP
          component will reflect the
          <a href="#limit-for-tokens-in-stable-diffusion"
            >similar limit as Stable Diffusion</a
          >
          of 75 tokens.
        </p>
        <h4>How is it private? Can anyone see what I'm looking up?</h4>
        <p>
          The tokenizer runs privately in your browser. Communications are
          limited to your browser and no connections/servers are used.
        </p>

        <h4>How much bandwidth does this use?</h4>
        <p>
          Loading tokenizers are around 2-3MB and are loaded into cache for
          further runs.
        </p>

        <h4>What tokenizers are each model using?</h4>
        <p>
          The tokenizers are usually associated with the text encoder model.
          Below is the associated text encoder which is assocated with a
          tokenizer.
        </p>
        <table>
          <thead>
            <tr>
              <th>Model</th>
              <th>Model source</th>
              <th>Text Encoder source</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <th scope="row">SD1</th>
              <td>runwayml/stable-diffusion-v1-5</td>
              <td>
                <ul>
                  <li>CLIP ViT/B (openai/clip-vit-base-patch32)</li>
                </ul>
              </td>
            </tr>
            <tr>
              <th scope="row">SD2</th>
              <td>stabilityai/stable-diffusion-2-1</td>
              <td>
                <ul>
                  <li>CLIP ViT/B (openai/clip-vit-base-patch32)</li>
                </ul>
              </td>
            </tr>
            <tr>
              <th scope="row">SDXL</th>
              <td>stabilityai/stable-diffusion-xl-base-1.0</td>
              <td>
                <ul>
                  <li>CLIP ViT/L (openai/clip-vit-large-patch14)</li>
                  <li>
                    OpenCLIP ViT/bigG (laion/CLIP-ViT-bigG-14-laion2B-39B-b160k)
                  </li>
                </ul>
              </td>
            </tr>
            <tr>
              <th scope="row">SD3</th>
              <td>stabilityai/stable-diffusion-3.5-large</td>
              <td>
                <ul>
                  <li>CLIP ViT/L (openai/clip-vit-large-patch14)</li>
                  <li>
                    OpenCLIP ViT/bigG (laion/CLIP-ViT-bigG-14-laion2B-39B-b160k)
                  </li>
                  <li>T5xxl (google/t5-v1_1-xxl)</li>
                </ul>
              </td>
            </tr>
            <tr>
              <th scope="row">Flux</th>
              <td>black-forest-labs/FLUX.1-schnell</td>
              <td>
                <ul>
                  <li>CLIP ViT/L (openai/clip-vit-large-patch14)</li>
                  <li>T5xxl (google/t5-v1_1-xxl)</li>
                </ul>
              </td>
            </tr>
            <tr>
              <th scope="row">Pixart</th>
              <td>PixArt-alpha/PixArt-XL-2-1024-MS</td>
              <td>
                <ul>
                  <li>T5xxl (google/t5-v1_1-xxl)</li>
                </ul>
              </td>
            </tr>
          </tbody>
        </table>
      </aside>
      <aside>
        <p>
          Some models use a tokenizer in another repository like T5 models refer
          to <code>google-t5/t5-small</code> for the tokenizer.
        </p>
      </aside>
    </article>

    <footer class="container">
      <h4>By <a href="https://rocker.boo">rockerBOO</a></h4>
      <nav>
        <ul>
          <li><a href="https://davelage.com">Dave Lage</a></li>
          <li>
            Source:
            <a href="https://github.com/rockerBOO/sd-tokenizer"
              >github.com/rockerBOO/sd-tokenizer</a
            >
          </li>

          <li>
            <a href="https://lora-inspector.rocker.boo">LoRA Inspector</a> -
            Inspector your LoRA models with metadata and tensor analysis.
          </li>
          <li>
            Tokenizers source:
            <a href="https://github.com/huggingface/tokenizers" rel="noopener"
              >https://github.com/huggingface/tokenizers</a
            >
          </li>
        </ul>
      </nav>
    </footer>
  </body>
</html>
